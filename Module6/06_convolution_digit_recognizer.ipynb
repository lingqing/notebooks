{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Dataflowr](https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png)](https://dataflowr.github.io/website/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUiFyiXKHovB"
   },
   "source": [
    "# [Module 6](https://dataflowr.github.io/website/modules/6-convolutional-neural-network/): Convolutions by examples\n",
    "\n",
    "We'll build our first Convolutional Neural Network (CNN) from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7v7DN_UKHovE"
   },
   "source": [
    "## 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQmaHvupHovF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math,sys,os,numpy as np\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkRdumG1HovK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/andy/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JlQUj5IHovP"
   },
   "source": [
    "Download MNIST data on disk and convert it to pytorch compatible formating.\n",
    "\n",
    "```torchvision.datasets``` features support (download, formatting) for a collection of popular datasets. The list of available datasets in ```torchvision``` can be found [here](http://pytorch.org/docs/master/torchvision/datasets.html).\n",
    "\n",
    "Note that the download is performed only once. The function will always check first if the data is already on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bv9h8isvHovQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data/MNIST/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = './data/MNIST/'\n",
    "torchvision.datasets.MNIST(root=root_dir,download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XW1c2YmiHovW"
   },
   "source": [
    "MNIST datasets consists of small images of hand-written digits. The images are grayscale and have size 28 x 28. There are 60,000 training images and 10,000 testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9MA5fqrHovZ"
   },
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST(root=root_dir, train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yaLXqwxaHovg"
   },
   "source": [
    "Define and initialize a data loader for the MNIST data already downloaded on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEyTgg0THovj"
   },
   "outputs": [],
   "source": [
    "MNIST_dataset = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f12CxE8XHovo"
   },
   "source": [
    "For the current notebook, we can format data as _numpy ndarrays_ which are easier to plot in matplotlib. The same operations can be easily performed on _pytorch Tensors_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UnuFlpWHovq"
   },
   "outputs": [],
   "source": [
    "images = train_set.data.numpy().astype(np.float32)/255\n",
    "labels = train_set.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpkGWiaqHovz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.return_types.max(\n",
       " values=tensor(99),\n",
       " indices=tensor(99)),\n",
       " tensor(99))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## just test code\n",
    "xin = torch.arange(0,100)\n",
    "print(xin[:10])\n",
    "torch.max(xin,dim=0), xin.max()\n",
    "# x2 = xin.reshape((10,10))\n",
    "# torch.max(x2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPtQoS_-HowJ"
   },
   "source": [
    "## 2. Data visualization\n",
    "\n",
    "For convenience we define a few functions for formatting and plotting our image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3lsBB7ZHowK"
   },
   "outputs": [],
   "source": [
    "# plot multiple images\n",
    "def plots(ims, interp=False, titles=None):\n",
    "    ims=np.array(ims)\n",
    "    mn,mx=ims.min(),ims.max()\n",
    "    f = plt.figure(figsize=(12,24))\n",
    "    for i in range(len(ims)):\n",
    "        sp=f.add_subplot(1, len(ims), i+1)\n",
    "        if not titles is None: sp.set_title(titles[i], fontsize=18)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none', vmin=mn,vmax=mx)\n",
    "\n",
    "# plot a single image\n",
    "def plot(im, interp=False):\n",
    "    f = plt.figure(figsize=(3,6), frameon=True)\n",
    "    plt.imshow(im, interpolation=None if interp else 'none')\n",
    "\n",
    "plt.gray()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfxlah5eHowW"
   },
   "outputs": [],
   "source": [
    "plot(images[5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VY5890I1Howd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEWipA5iHown"
   },
   "outputs": [],
   "source": [
    "plots(images[5000:5005], titles=labels[5000:5005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyc9VagHHo0S"
   },
   "source": [
    "## 3. A simple classifier\n",
    "\n",
    "In this section we will construct a basic binary classifier.\n",
    "Our classifier will tell us whether a given image depicts a _one_ or an _eight_. \n",
    "\n",
    "We fetch all images from the _eight_ class and from the _one_ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogJld-3qJDpW"
   },
   "outputs": [],
   "source": [
    "n=len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Itlgssc9Ho0U"
   },
   "outputs": [],
   "source": [
    "eights=[images[i] for i in range(n) if labels[i]==8]\n",
    "ones=[images[i] for i in range(n) if labels[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGY2P44IHo0X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5851, 6742)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eights), len(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFmNNrKAHo0f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAACUCAYAAACa7UEyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZgUlEQVR4nO3debAU1dnH8ecIiEIIIiBeNs2bRMKSiikpgqUgxhVUBKTcgIABDPJKQgJEFiGpBAyJFCjghgEub9xKRAQFg0JA6w1vLAERERCEgiAii6xuAbnn/YNr1X1Oz+3puTN3pnvO91M1Bb+e6e5zmYfmOD59xlhrBQAAAPDBGYUeAAAAAJAvTH4BAADgDSa/AAAA8AaTXwAAAHiDyS8AAAC8weQXAAAA3shq8muMud4Y84Ex5kNjzOhcDQrFh1pBFNQJoqJWEAV1gpSstVV6iEgNEdkuIv8lImeKyLsi0ibNPpZH8Tyqq1YK/XPxyPnjANcUHlEeXFN4RHxwTeER6VHZ+5zNJ78dRORDa+0Oa+0JEXlORG7O4ngoXtSK33ZFfB11gqioFb9xTUFWspn8NhOR3RXyR+XbFGPM3caYNcaYNVmcC8mWtlaoEwjXFETHNQVRcE1BSjWr+wTW2lkiMktExBhjq/t8SCbqBFFRK4iCOkFU1Ip/svnkd4+ItKiQm5dvA1zUCqKgThAVtYIoqBOklM3k920R+b4x5jvGmDNF5HYRWZybYaHIUCuIgjpBVNQKoqBOkFKV2x6stV8bY+4VkWVy+o7KOdba93M2MhQNagVRUCeIilpBFNQJKmPKl/bIz8nopSkq1lpTHcelTorOWmtt++o4MLVSXLimICKuKYiksmsK3/AGAAAAbzD5BQAAgDeY/AIAAMAbTH4BAADgDSa/AAAA8AaTXwAAAHiDyS8AAAC8weQXAAAA3mDyCwAAAG9U+euNAQDJNXLkSJV/97vfqTxhwgSVp02bVu1jAoB84JNfAAAAeIPJLwAAALzB5BcAAADeoOc3IerWravyqlWrVG7atKnKl112mco7d+6sjmEhA23atFF5+PDhKpeUlKh84403qrxo0aLAMVevXh16zlmzZql85MiRNKNEHLVt21blmjXDL93Hjx8PbNuxY0foPnXq1FG5X79+KtPzW3hnnKE/r3r00UdVvuSSS1Tev3+/ylu2bFF5wYIFgXNs3rxZ5cOHD2c8TiDu+OQXAAAA3mDyCwAAAG8w+QUAAIA36PnNA7cft3Hjxmn3cfusrrzySpXd3q4PPvhA5U8//TSTISIPBg0apPLAgQNDX19WVqbyTTfdFHhNqm0V/fa3v1V53LhxKj/xxBOh+yM/GjZsqPJDDz2k8i233KJy7dq1VbbWqnzo0KHAOTp06JDRmLhPIH7ca4JbJxMnTlT5hhtuUNmtmz59+gTO4fYVX3rppSpv37490lgRLy1atFB58ODBoa/v37+/yi1btkx7jl/+8pcqu/ecDBs2TGV3bfH3339f5c6dOwfOceLEibTjiIJPfgEAAOANJr8AAADwBpNfAAAAeMO4vWLVejJj8neyHGrXrp3Kbl/LBRdcELr/RRddpHKU3pnJkyer7K4R26NHD5VXrFih8m233aZydfQAW2tNzg8qya2TdHbv3q2y2wvueuedd1Tes2dPxuf86U9/qvKaNWtUdnvJq8laa2376jhwUmvl8ssvV/n3v/+9yl26dAnd3xj9Vy/Kdfziiy9W+ZlnnlH5/PPPV7lVq1Yq52O9V5+vKQ0aNAhsc+ugUaNGKrv/Lrg9lf/5z39U7t69u8qp+rr/8Y9/qPz555+HnrNAuKY4zjrrLJXdOcDYsWNV/t73vlftY8rUyZMnVXbvhRAJ1mM6lV1T+OQXAAAA3mDyCwAAAG8w+QUAAIA3WOc3ArdvMt36rC637+qpp54KPb6IyOjRo0OP6fb4lZaWqsw6v8mzbds2lbt166by/v370x6jSZMmKrs9vj/60Y9UHjBggMpLlixR+cCBA2nPiXBdu3YNbHv++edVPvvss0OP8dhjj4U+f8UVV6h89dVXB17jrvn8gx/8QOWDBw+qnI8eX5/UqlVL5QkTJqg8dOjQwD6p+oAz4dbVSy+9pHK/fv0C+xw9elRl956V7373uyqz7m/+NWvWLLDttddeU9n9++06fvy4ynPnzlXZ7Qdv3bp14Bjp1gpO55///KfK999/v8qZ9vdmgk9+AQAA4A0mvwAAAPAGk18AAAB4g55fh7vepojIqFGjQveZN2+eym6f5JQpU0Kfd9ffFBFZtmyZyu76ju4xXnjhhdAxIv7c/qYoPb5uT6Dbg5VuLeHZs2ervGDBApVvvfXWtGNAuKVLlwa2lZWVqbx+/XqVr7/+epXdv+/u+q/ufQUzZswInNPdp0aNGir//e9/D+yD3JkzZ47Kt99+u8pbtmwJ7OP2BbvX+Xr16qncu3dvld17R84880yVU/UUu+vFuv3m9Pjmn9vj6/b3igR7fLdu3ary9OnTVXavS7t27VK5du3aoftXxRdffKHyn/70J5XffPPNrM8RFZ/8AgAAwBtMfgEAAOCNtJNfY8wcY8x+Y8zGCtvONca8bozZVv5rduuxoChQK4iCOkFU1AqioE6QqSg9v6UiMlNE/qfCttEissJaO9kYM7o835f74eVf3bp1A9vctRLd3phx48apvHfv3tBzuN+p7X7ntohI48aNVXb7Qd3e5K+++ir0nHlSKh7VSq61aNFC5c6dO6u8ceNGcb366qsqt2+f2dfdu72nCxcuzGj/KioVj+rE/TMWCa7TvXbtWpU/++yz0GOuWrVK5Zo19aU8Sv+uu6bzz3/+87T7FECpJLRW3DW13R7f+fPnq3znnXdmfI59+/apPHnyZJUff/xxld3e0VR9nG7/+F/+8peMx1UApZLQOonCnSOkWsPXrYUbbrhB5R07dmR0Tvffn0GDBmW0fyp9+/ZV2f33K5/SfvJrrX1TRA45m28WkW/u8ponIj1yOywkEbWCKKgTREWtIArqBJmqas9vE2vtNx9vfiIiTcJeDK9RK4iCOkFU1AqioE5QqayXOrPWWmOMrex5Y8zdInJ3tudB8oXVCnWCb3BNQVRcUxAF1xS4qvrJ7z5jTImISPmvlS5Iaq2dZa1tb63NrBkRxSJSrVAn3uOagqi4piAKrimoVFU/+V0sIv1FZHL5r4tyNqICS/VlEe6C823atFHZvclg6NChKtevX1/lqVOnquw2pouIHDqk25cmTZqksrvweIwVba3kWsOGDVVeuXJlzs+xc+dOld2bWZ599tmcnzOioq0T90tuRERGjBihsnuzWevWrVUeNmxY6Dnmzp2bdhzujbjuFygkSCJqxb1xzL3J0f1yo29/+9uBYxw7diyrMZw4cULlIUOGqNypU6fAPvfcc4/K6W7gjrFE1EmufPnllyq7X4yTTvPmzVW+7bbbsh7Tyy+/rPLy5cuzPmauRFnq7FkR+T8RaWWM+cgYM1BOF9M1xphtInJ1eYbnqBVEQZ0gKmoFUVAnyFTaT36ttXdU8tRVOR4LEo5aQRTUCaKiVhAFdYJM8Q1vAAAA8IZx+5Cq9WQhd1vGRe3atQPbZs6cqbLbn/fxxx+r7PbnTZs2TeWWLVumHcfw4cNVnjFjRtp98s1aa6rjuEmok6rYvXu3yk2bNs35Od577z2V3X71Tz/9VOWTJ0/mfAwprK2uG0mSUCu1atUKbHPvLUjV91/RwYMHVTZG/9Vz+8VT9WnefPPNKq9bty70nIVQzNeUP//5zyqPGjVKZfc9FhEZP368ym5vt9vT65o1a5bK1113ncojR44M7ON++UZMeXVNueKKK1ResGBB4DUNGugvsFu8eLHK/fv3V9ntJ3/ttddUvuqqzD80X716tcrdunVT+fjx4xkfM1uVXVP45BcAAADeYPILAAAAbzD5BQAAgDey/oa3YuOuzSiSfq1Ft3fT7cdx+/PcPuvZs2cHjvnSSy+FnhPx16pVK5XPPvvsrI5XVlYW2Hb33fpLiRYuXKjykSNHsjonspeqr/pnP/uZymPHjlXZXQe4UaNGKrvXlP379fr9N910U+Cc69evTztWVB/3Pf7b3/6mcqq1291tPXv2VHnAgAEqu2v0uq/v2LGjytu3b698wIiNN954Q+U+ffoEXrN06VKVu3fvrnJpaanKEydOVLlevXoZj8v99+WBBx5QuRA9vlHxyS8AAAC8weQXAAAA3mDyCwAAAG/Q8xvBrl27cno8tzdnypQpgde4a8IiXmrWDP7VufLKK1V+8sknVXbXYXS5/VHLli1TedKkSYF9NmzYEHpMxNPRo0dV/te//qWy29PrOuMM/bnFJ598ojL9vfFz6tQplTdu3Khyp06dAvsMHjxY5QcffFBld415l7tWKz2+xeHNN98MbHPX8Z06darK7jrfbk4n1f0jffv2VfnVV1/N6JiFxCe/AAAA8AaTXwAAAHiDyS8AAAC8Qc+vo0aNGoFtbi9Wun4815IlS1ROtQYn4u3CCy9U2V1fV0Tkvvvuy+ocY8aMUTnVup8oDpdcconK06dPV9ldC/zAgQMqu9egFi1aqNyuXbvAOd0eU8Sfe9/Addddp3KvXr1C92/btq3KK1euzM3AUFBffvllYNtTTz2lsntfQabfHXD48GGV3Z5ikWT1+Lr45BcAAADeYPILAAAAbzD5BQAAgDfo+XU899xzgW1uX5Xbj5dOpq9H4bk9mS+++KLKzZs3z/k5t23blvNjovAuvvjiwDa3/66kpETlvXv3quzeJ3DnnXeq/Jvf/Eble++9N3DOIUOGpBsqCshdu1lE5PHHH1e5Z8+eKg8cOFDlfv36qez2kn/99dehx0dy1a1bV+XevXtndbz58+er7N67lHR88gsAAABvMPkFAACAN5j8AgAAwBve9fw2bdpU5bvuukvlW265JbCP27O7bt06ld99993QY5533nkZjxP55fZluj2Zbt2kcurUKZVfeeUVlTP9LnUUh2XLlgW2NWzYUGW3x9etlfXr16vs9vy6OnbsGNjWoEEDld11PFFYd9xxR2DboEGDVJ40aZLKc+fOVfmFF15QefXq1Sq7a4lv2bIlcM5Vq1alHSsKq06dOoFt7jWhb9++occ4cuSIym4/eO3atas2uITgk18AAAB4g8kvAAAAvMHkFwAAAN5g8gsAAABveHfD21VXXaXyH/7wh7T73H///SrPnDlT5R49eqjs3vC2adOmDEaIQnj66adVTneD29KlSwPbpkyZorJ7Ex03vPlhzpw5Kjdq1CjwGvcmWvdLKdybajNVr169wLazzjorq2Mit9y6mDFjRtp9Zs2aFfr88ePHVXZvmFu+fLnKf/zjHwPHcL9I4+DBg2nHhfxyb1wUERk7dmzoPu5N3O4x3HlNs2bNqja4hOCTXwAAAHiDyS8AAAC8weQXAAAA3ij6nt8uXbqoPH369NDXd+/ePbDN7ZM6//zzVZ4wYULoMXfu3Bn6PPLPXQD8oosuUnnNmjUqX3311Sp/9dVXgWOePHlS5V/84hfZDBEJ0a9fP5X79++v8hlnBD9jePjhh1VetGhRRuesW7euysYYld0vxRAJfpEGCuuaa65R+Zxzzgm8Zvv27Srv27cvo3O89dZbKv/6179WOVUPcZ8+fVR2axX5N378eJWHDh2adp+RI0eqPHv2bJWPHTuW/cASjE9+AQAA4A0mvwAAAPBG2smvMaaFMWalMWaTMeZ9Y8yvyrefa4x53RizrfzXBumOheJFnSAqagVRUSuIgjpBpqL0/H4tIiOsteuMMfVEZK0x5nURGSAiK6y1k40xo0VktIjcV31DrRq3r6p+/foqv/HGGyq/8sorgWPUqlVL5RtvvDH0mG7/3YEDB6INNtliXSetWrVS2V3f2e3LPHXqlMru+plR9OrVK/T5/fv3h+YiFutayVTXrl1VdtfwPXHiRGCf119/PaNztG3bVuUhQ4ao/O9//1tld93gBCuqWqnIvc8glSeeeELlVLWUiZdffjnt8dx/zxKiqOqkc+fOKru92qneI/e9/etf/6qy+29Y48aNVS4pKVH5448/jjbYhEr7ya+1dq+1dl3574+LyGYRaSYiN4vIvPKXzRORHtU0RiQAdYKoqBVERa0gCuoEmcpotQdjzIUi8mMReUtEmlhrv7l9+BMRaVLJPneLyN1ZjBEJQ50gKmoFUWVaK9SJn7imIIrIN7wZY74lIgtEZLi1Vq2RYU//fz6baj9r7SxrbXtrbfusRopEoE4QFbWCqKpSK9SJf7imIKpIn/waY2rJ6YJ62lr7YvnmfcaYEmvtXmNMiYjEsmGxrKxMZbcfz81uf6+ISI8ePVR21z08fPiwym6vzWOPPRZprEkX5zpxe6TcfifX0qVLQ59P1a/nru9as2b4X6+FCxeqvGHDhtDXF5M410qmWrduHfr8Z599Ftjmrgl97bXXqtymTRuVH3jggdBzvPPOOyoX05q+xVQrmXr77bdzejy3rzPdNSpJklwn7rrdixcvVrlevXoq79mzJ3AMd33mzz//PPScDz30kMruNWfJkiWh+yddlNUejIjMFpHN1tqpFZ5aLCLf/GvfX0QyW6UdRYU6QVTUCqKiVhAFdYJMRfnPvstEpJ+IvGeMWV++bayITBaR540xA0Vkl4jcWi0jRFJQJ4iKWkFU1AqioE6QkbSTX2vt/4qIqeTpq3I7HCQVdYKoqBVERa0gCuoEmSqehp9KnHfeeaHPu2vwplp/s1OnTqHHuOuuu1R219tD4XXp0kXlOnXqhL7eXVfxJz/5icrt2rUL7NOyZcvQY7prSo8ePTr09UiGTZs2qfzDH/5Q5XPOOSewT7qe8nTc9V/HjRuX1fGQf+5a4qnW3N2xY0dW53DvTbjvPr3Erbu+uUhx9YsnxYgRI1R2e3y/+OILlQcOHBg4RroeX3ee0rNnT5XddX3de5eKDV9vDAAAAG8w+QUAAIA3mPwCAADAG0Xf87t58+bQ53v37q3y6RVTtEOHDqn8yCOPqLx8+fIqjg754vZYDh8+XOUmTfQX/7h9mt26dcv4nG6f1tSpU1U+dkytwY6Ect/Xjz76SOWRI0emPcaaNWtUdnvSu3fvrvLOnTszGCHiaN68eSoPGzYs8Br3ulVaWhp6zEaNGqns9vi6faETJ04MHOPJJ58MPQdyL909KCtXrlS5bdu2gde423r16qVyhw4dVHa/0+Cee+5R+cMPPwwdU9LxyS8AAAC8weQXAAAA3mDyCwAAAG8Ya23+TmZM/k5WrkGDBioPHjxY5fHjx6vs9t6JBL9ne9q0aTkaXbJZaytbVDwr+agTd51ed33ndOtDp7Js2TKVH3zwQZXdvi2PrLXWtq+OAxfimoLqk+RrSrauvfbawLZnnnlG5XPPPTejYy5cuFDlMWPGqLx169aMjhcjRXVNmTx5ssqjRo2q9nM+/PDDoed016FOqsquKXzyCwAAAG8w+QUAAIA3mPwCAADAG0x+AQAA4I2iv+EN1cfnm1OQkaK6OQXVh2sKIiqqa0r9+vVVdr9YKwp3n0cffVTl+fPnq7xp0yaVy8rKMj5nEnDDGwAAALzH5BcAAADeYPILAAAAb9Qs9AAAAAB8dfToUZVr1KhRoJH4g09+AQAA4A0mvwAAAPAGk18AAAB4g8kvAAAAvMHkFwAAAN5g8gsAAABvMPkFAACAN/K9zu9BEdklIo3Kfx9njDHcBdV47CTViUgyxkmtFB5jDJePOhHhfciVYq8V3oPcKdQ4K60TY63N50BOn9SYNdba9nk/cQYYY+El5edLwjiTMMZsJOHnY4zxkISfkTEWXhJ+viSMUSSe46TtAQAAAN5g8gsAAABvFGryO6tA580EYyy8pPx8SRhnEsaYjST8fIwxHpLwMzLGwkvCz5eEMYrEcJwF6fkFAAAACoG2BwAAAHiDyS8AAAC8kdfJrzHmemPMB8aYD40xo/N57jDGmDnGmP3GmI0Vtp1rjHndGLOt/NcGBR5jC2PMSmPMJmPM+8aYX8VxnLkSx1qhTuInjnUiQq3EEbVS5fF5VSci8ayVuNdJ+XgSUyt5m/waY2qIyCMi0lVE2ojIHcaYNvk6fxqlInK9s220iKyw1n5fRFaU50L6WkRGWGvbiEhHEfnv8j+/uI0zazGulVKhTmIjxnUiQq3ECrWSFW/qRCTWtVIq8a4TkSTVirU2Lw8RuVREllXIY0RkTL7OH2F8F4rIxgr5AxEpKf99iYh8UOgxOuNdJCLXxH2cxVYr1El8HnGuE2olXg9qhTophlpJUp3EvVby2fbQTER2V8gflW+LqybW2r3lv/9ERJoUcjAVGWMuFJEfi8hbEuNxZiFJtRLbP3/qJHZi+x5QK7ETy/fAgzoRSVatxPY9iHutcMNbBPb0f67EYk04Y8y3RGSBiAy31h6r+FycxumjOP35UyfxFqf3gFqJt7i8B9RJvMXpPUhCreRz8rtHRFpUyM3Lt8XVPmNMiYhI+a/7CzweMcbUktMF9bS19sXyzbEbZw4kqVZi9+dPncRW7N4DaiW2YvUeeFQnIsmqldi9B0mplXxOft8Wke8bY75jjDlTRG4XkcV5PH+mFotI//Lf95fTvSsFY4wxIjJbRDZba6dWeCpW48yRJNVKrP78qZPY1olIzN4DaoVaicKzOhFJVq3E6j1IVK3kufm5m4hsFZHtIjKu0A3PFcb1rIjsFZGTcrq/Z6CINJTTdyVuE5HlInJugcd4uZz+XwUbRGR9+aNb3MZZzLVCncTvEcc6oVbi+aBWqJMk10rc6yRptcLXGwMAAMAb3PAGAAAAbzD5BQAAgDeY/AIAAMAbTH4BAADgDSa/AAAA8AaTXwAAAHiDyS8AAAC88f+Kx4jk0SxUSgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x1728 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x1728 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots(eights[:5])\n",
    "plots(ones[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Mxs-x1pHo0l"
   },
   "source": [
    "We keep the first 1000 digits for the test set and we average all the remaining digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEA8brOGHo0n"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "raws8 =  np.mean(eights[1000:],axis=0)\n",
    "\n",
    "plot(raws8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipyVJA9OMUH-"
   },
   "source": [
    "We now do the same thing with the ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aa3a1Le2Ho07"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "raws1 =  np.mean(ones[1000:],axis=0)\n",
    "\n",
    "plot(raws1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6UJ-NicMUIA"
   },
   "source": [
    "We built a 'typical representative' of the eights and a 'typical representative' of the ones. Now for a new sample from the test set, we compute the distance between this sample and our two representatives and classify the sample with the label of the closest representative.\n",
    "\n",
    "For the distance between images, we just take the pixelwise squared distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZML33QNHo1G"
   },
   "outputs": [],
   "source": [
    "# sum of squared distance\n",
    "def sse(a,b): return ((a-b)**2).sum()\n",
    "\n",
    "# return 1 if closest to 8 and 0 otherwise\n",
    "def is8_raw_n2(im): return 1 if sse(im,raws1) > sse(im,raws8) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMqqBDrMMUID"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "nb_8_predicted_8, nb_1_predicted_8 = [np.array([is8_raw_n2(im) for im in ims]).sum() for ims in [eights[:1000],ones[:1000]]]\n",
    "\n",
    "nb_8_predicted_1, nb_1_predicted_1 = [np.array([(1-is8_raw_n2(im)) for im in ims]).sum() for ims in [eights[:1000],ones[:1000]]]\n",
    "\n",
    "# just to check \n",
    "print(nb_8_predicted_1+nb_8_predicted_8, nb_1_predicted_1+nb_1_predicted_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFUntUqdHo1e"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/1024px-Precisionrecall.svg.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "source [wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaYe8malHo1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 8: 0.9712460063897763 recall 8: 0.912\n",
      "precision 1: 0.9170593779453345 recall 1: 0.973\n",
      "accuracy : 0.9425\n"
     ]
    }
   ],
   "source": [
    "def compute_scores(nb_8_predicted_8,nb_8_predicted_1,nb_1_predicted_1,nb_1_predicted_8):\n",
    "    Precision_8 = nb_8_predicted_8/(nb_8_predicted_8+nb_1_predicted_8)\n",
    "    Recall_8 = nb_8_predicted_8/(nb_8_predicted_1+nb_8_predicted_8)\n",
    "    Precision_1 = nb_1_predicted_1/(nb_1_predicted_1+nb_8_predicted_1)\n",
    "    Recall_1 = nb_1_predicted_1/(nb_1_predicted_1+nb_1_predicted_8)\n",
    "    return Precision_8, Recall_8, Precision_1, Recall_1\n",
    "\n",
    "Precision_8, Recall_8, Precision_1, Recall_1 = compute_scores(nb_8_predicted_8,nb_8_predicted_1,nb_1_predicted_1,nb_1_predicted_8)\n",
    "\n",
    "print('precision 8:', Precision_8, 'recall 8:', Recall_8)\n",
    "print('precision 1:', Precision_1, 'recall 1:', Recall_1)\n",
    "print('accuracy :', (Recall_1+Recall_8)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7UtzI0uMUIH"
   },
   "source": [
    "This is our baseline for our binary classification task. Now your task will be to do better with convolutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unXh3clNHoyM"
   },
   "source": [
    "## 4. Filters and convolutions\n",
    "\n",
    "Let start with this visual explanation of [Interactive image kernels](http://setosa.io/ev/image-kernels/)\n",
    "\n",
    "In some fields, convolution or filtering can be better understood as _correlations_. \n",
    "In practice we slide the filter matrix over the image (a bigger matrix) always selecting patches from the image with the same size as the filter. We compute the dot product between the filter and the image patch and store the scalar response which reflects the degree of similarity/correlation between the filter and image patch.\n",
    "\n",
    "Here is a simple 3x3 filter, ie a 3x3 matrix (see [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator) for more examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWiIsQ38HoyP"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top=[[-1,-1,-1],\n",
    "     [ 1, 1, 1],\n",
    "     [ 0, 0, 0]]\n",
    "\n",
    "plot(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dr63ciIMUIK"
   },
   "source": [
    "We now create a toy image, to understand how convolutions operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jipouDFOHoyn"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross = np.zeros((28,28))\n",
    "cross += np.eye(28)\n",
    "for i in range(4):\n",
    "    cross[12+i,:] = np.ones(28)\n",
    "    cross[:,12+i] = np.ones(28)\n",
    "\n",
    "plot(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0j7I3OKHoyt"
   },
   "source": [
    "Our `top` filter should highlight top horizontal border in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXc6nlfYHoyx"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.ndimage.filters import convolve, correlate\n",
    "\n",
    "corr_cross = correlate(cross,top)\n",
    "\n",
    "plot(corr_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mi2j-hFHoy3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reflect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Multidimensional correlation.\n",
      "\n",
      "The array is correlated with the given kernel.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input : array_like\n",
      "    The input array.\n",
      "weights : ndarray\n",
      "    array of weights, same number of dimensions as input\n",
      "output : array or dtype, optional\n",
      "    The array in which to place the output, or the dtype of the\n",
      "    returned array. By default an array of the same dtype as input\n",
      "    will be created.\n",
      "mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n",
      "    The `mode` parameter determines how the input array is extended\n",
      "    beyond its boundaries. Default is 'reflect'. Behavior for each valid\n",
      "    value is as follows:\n",
      "\n",
      "    'reflect' (`d c b a | a b c d | d c b a`)\n",
      "        The input is extended by reflecting about the edge of the last\n",
      "        pixel. This mode is also sometimes referred to as half-sample\n",
      "        symmetric.\n",
      "\n",
      "    'constant' (`k k k k | a b c d | k k k k`)\n",
      "        The input is extended by filling all values beyond the edge with\n",
      "        the same constant value, defined by the `cval` parameter.\n",
      "\n",
      "    'nearest' (`a a a a | a b c d | d d d d`)\n",
      "        The input is extended by replicating the last pixel.\n",
      "\n",
      "    'mirror' (`d c b | a b c d | c b a`)\n",
      "        The input is extended by reflecting about the center of the last\n",
      "        pixel. This mode is also sometimes referred to as whole-sample\n",
      "        symmetric.\n",
      "\n",
      "    'wrap' (`a b c d | a b c d | a b c d`)\n",
      "        The input is extended by wrapping around to the opposite edge.\n",
      "\n",
      "    For consistency with the interpolation functions, the following mode\n",
      "    names can also be used:\n",
      "\n",
      "    'grid-mirror'\n",
      "        This is a synonym for 'reflect'.\n",
      "\n",
      "    'grid-constant'\n",
      "        This is a synonym for 'constant'.\n",
      "\n",
      "    'grid-wrap'\n",
      "        This is a synonym for 'wrap'.\n",
      "cval : scalar, optional\n",
      "    Value to fill past edges of input if `mode` is 'constant'. Default\n",
      "    is 0.0.\n",
      "origin : int or sequence, optional\n",
      "    Controls the placement of the filter on the input array's pixels.\n",
      "    A value of 0 (the default) centers the filter over the pixel, with\n",
      "    positive values shifting the filter to the left, and negative ones\n",
      "    to the right. By passing a sequence of origins with length equal to\n",
      "    the number of dimensions of the input array, different shifts can\n",
      "    be specified along each axis.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "result : ndarray\n",
      "    The result of correlation of `input` with `weights`.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "convolve : Convolve an image with a kernel.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Correlation is the process of moving a filter mask often referred to\n",
      "as kernel over the image and computing the sum of products at each location.\n",
      "\n",
      ">>> from scipy.ndimage import correlate\n",
      ">>> input_img = np.arange(25).reshape(5,5)\n",
      ">>> print(input_img)\n",
      "[[ 0  1  2  3  4]\n",
      "[ 5  6  7  8  9]\n",
      "[10 11 12 13 14]\n",
      "[15 16 17 18 19]\n",
      "[20 21 22 23 24]]\n",
      "\n",
      "Define a kernel (weights) for correlation. In this example, it is for sum of\n",
      "center and up, down, left and right next elements.\n",
      "\n",
      ">>> weights = [[0, 1, 0],\n",
      "...            [1, 1, 1],\n",
      "...            [0, 1, 0]]\n",
      "\n",
      "We can calculate a correlation result:\n",
      "For example, element ``[2,2]`` is ``7 + 11 + 12 + 13 + 17 = 60``.\n",
      "\n",
      ">>> correlate(input_img, weights)\n",
      "array([[  6,  10,  15,  20,  24],\n",
      "    [ 26,  30,  35,  40,  44],\n",
      "    [ 51,  55,  60,  65,  69],\n",
      "    [ 76,  80,  85,  90,  94],\n",
      "    [ 96, 100, 105, 110, 114]])\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/py37/lib/python3.7/site-packages/scipy/ndimage/filters.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "?correlate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OdlQ_Kz8MUIS"
   },
   "source": [
    "What is done on the border of the image? \n",
    "\n",
    "## Padding\n",
    "\n",
    "![padding](https://dataflowr.github.io/notebooks/Module6/img/padding_conv.gif)\n",
    "\n",
    "source: [Convolution animations](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPAEi-4gHoy-"
   },
   "outputs": [],
   "source": [
    "# to see the role of padding\n",
    "corr_cross = correlate(cross,top, mode='constant')\n",
    "plot(corr_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSTy0Ka1HozC"
   },
   "outputs": [],
   "source": [
    "corrtop = correlate(images[5000], top)\n",
    "plot(corrtop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kp8ArumHozJ"
   },
   "source": [
    "By rotating the filter with 90 degrees and calling the ```convolve``` function we get the same response as with the previously called ```correlate``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2m8hh6XHozL"
   },
   "outputs": [],
   "source": [
    "np.rot90(top, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBratD5iHozV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convtop = convolve(images[5000], np.rot90(top,2))\n",
    "plot(convtop)\n",
    "np.allclose(convtop, corrtop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqFCxFXoHozd"
   },
   "source": [
    "Let's generate a few more variants of our simple 3x3 filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A48nt8YqHoze"
   },
   "outputs": [],
   "source": [
    "straights=[np.rot90(top,i) for i in range(4)]\n",
    "plots(straights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cv5eeO7gHozn"
   },
   "source": [
    "We proceed similarly to generate a set of filters with a different behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhJFADUoHozr"
   },
   "outputs": [],
   "source": [
    "br=[[ 0, 0, 1],\n",
    "    [ 0, 1,-1.5],\n",
    "    [ 1,-1.5, 0]]\n",
    "\n",
    "diags = [np.rot90(br,i) for i in range(4)]\n",
    "plots(diags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhGJF9gNHoz0"
   },
   "source": [
    "We can compose filters to obtain more complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9ETAdFKHoz1"
   },
   "outputs": [],
   "source": [
    "rots = straights + diags\n",
    "corrs_cross = [correlate(cross, rot) for rot in rots]\n",
    "plots(corrs_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GnSnG1HHoz5"
   },
   "outputs": [],
   "source": [
    "rots = straights + diags\n",
    "corrs = [correlate(images[5000], rot) for rot in rots]\n",
    "plots(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plV6mfsjHoz-"
   },
   "source": [
    "Next we illustrate the effect of downsampling.\n",
    "We select the most basic downsampling technique: __max pooling__. We keep only the maximum value for sliding windows of size ```7x7```.\n",
    "__Max pooling__ is a handy technique with a few useful perks:\n",
    "- since it selects the maximum values it ensures invariance to translations\n",
    "- reducing the size is helpful since data becomes more compact and easier to compare\n",
    "- we will see later in this course that since max pooling reduces the size of our images, the operations performed later on in the network have bigger receptive field / concern a bigger patch in the input image and allow the discovery of higher level patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfR3u9FYHo0E"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_333582/2884827413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mblock_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corrs' is not defined"
     ]
    }
   ],
   "source": [
    "import skimage\n",
    "\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "def pool(im): return block_reduce(im, (7,7), np.max)\n",
    "\n",
    "plots([pool(im) for im in corrs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ud1VbsiRHo1s"
   },
   "source": [
    "We now build a classifier with convolutions.\n",
    "\n",
    "To this end we select a set of training images depicting _eights_ and _ones_, we convolve them with our set of filters, pool them and average them for each class and filter. We will thus obtain a set of _representative_ signatures for _eights_ and for _ones_. \n",
    "Given a new test image we compute its features by convolution and pooling with the same filters and then compare them with the _representative_ features. The class with the most _similar_ features is chosen as prediction.\n",
    "\n",
    "\n",
    "We keep 1000 images of _eight_ for the test set and use the remaining ones for the training: we convolve them with our bank of filters, perform max pooling on the responses and store them in ```pool8```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptgnsYPmHo1u"
   },
   "outputs": [],
   "source": [
    "pool8 = [np.array([pool(correlate(im, rot)) for im in eights[1000:]]) for rot in rots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoxKz3LNHo10"
   },
   "outputs": [],
   "source": [
    "len(pool8), pool8[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUSDBnm9Ho13"
   },
   "source": [
    "We plot the result of the first filter+pooling on the first 5 _eights_ in our set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlqkFm1RHo14"
   },
   "outputs": [],
   "source": [
    "plots(pool8[0][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5i4oiuoeHo17"
   },
   "source": [
    "For the 4 first _eights_ in our set, we plot the result of the 8 filters+pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Av_HAmtJHo17"
   },
   "outputs": [],
   "source": [
    "plots([pool8[i][0] for i in range(8)])\n",
    "plots([pool8[i][1] for i in range(8)])\n",
    "plots([pool8[i][2] for i in range(8)])\n",
    "plots([pool8[i][3] for i in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqDZeWY6Ho1-"
   },
   "source": [
    "We normalize the data in order to smoothen activations and bring them to similar ranges of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5wmCZY9Ho2A"
   },
   "outputs": [],
   "source": [
    "def normalize(arr): return (arr-arr.mean())/arr.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-bzBjC6Ho2D"
   },
   "source": [
    "Next we compute the average _eight_ by averaging all responses for each filter from _rots_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ldr9Cpx5Ho2D"
   },
   "outputs": [],
   "source": [
    "filts8 = np.array([ims.mean(axis=0) for ims in pool8])\n",
    "filts8 = normalize(filts8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZt78eGSHo2H"
   },
   "source": [
    "We should obtain a set of canonical _eights_ responses for each filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UdpsK6AHo2K"
   },
   "outputs": [],
   "source": [
    "plots(filts8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HaAjI_uHo2O"
   },
   "source": [
    "We proceed similarly with training samples from the _one_ class and plot the canonical _ones_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6baq8YKHo2P"
   },
   "outputs": [],
   "source": [
    "pool1 = [np.array([pool(correlate(im, rot)) for im in ones[1000:]]) for rot in rots]\n",
    "filts1 = np.array([ims.mean(axis=0) for ims in pool1])\n",
    "filts1 = normalize(filts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eq2CtFfAHo2T"
   },
   "outputs": [],
   "source": [
    "plots(filts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bu97H3bNHo2Z"
   },
   "source": [
    "Do you notice any differences between ```filts8``` and ```filts1```? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Krvaio26Ho2a"
   },
   "source": [
    "We define a function that correlates a given image with all filters from ```rots``` and max pools the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgnZzimPHo2a"
   },
   "outputs": [],
   "source": [
    "def pool_corr(im): return np.array([pool(correlate(im, rot)) for rot in rots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vFjsSxcHo2c"
   },
   "outputs": [],
   "source": [
    "plots(pool_corr(eights[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Td0bFQ04Ho2j"
   },
   "outputs": [],
   "source": [
    "#check \n",
    "plots([pool8[i][0] for i in range(8)])\n",
    "np.allclose(pool_corr(eights[1000]),[pool8[i][0] for i in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2cJdyL2Ho2u"
   },
   "outputs": [],
   "source": [
    "# function used for a voting based classifier that will indicate which one of the \n",
    "# two classes is most likely given the sse distances\n",
    "# n2 comes from norm2\n",
    "# is8_n2 returns 1 if it thinks it's an eight and 0 otherwise\n",
    "def is8_n2(im): return 1 if sse(pool_corr(im),filts1) > sse(pool_corr(im),filts8) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDFMHV3nHo2y"
   },
   "source": [
    "We perform a check to see if our function actually works. We correlate the an image of _eight_ with ```filts8``` and ```filts1```. It should give smaller distance for the _eights_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_eFq1EWHo2z"
   },
   "outputs": [],
   "source": [
    "sse(pool_corr(eights[0]), filts8), sse(pool_corr(eights[0]), filts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNINKzvKHo24"
   },
   "outputs": [],
   "source": [
    "plot(eights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roCmorVtHo29"
   },
   "source": [
    "We now test our classifier on the 1000 images of _eights_ and 1000 images of _ones_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEPN_aPbHo3K"
   },
   "outputs": [],
   "source": [
    "nb_8_predicted_8, nb_1_predicted_8 = [np.array([is8_n2(im) for im in ims]).sum() for ims in [eights[:1000],ones[:1000]]]\n",
    "\n",
    "nb_8_predicted_1, nb_1_predicted_1 = [np.array([(1-is8_n2(im)) for im in ims]).sum() for ims in [eights[:1000],ones[:1000]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FO4sLIjSHo3O"
   },
   "outputs": [],
   "source": [
    "Precisionf_8, Recallf_8, Precisionf_1, Recallf_1 = compute_scores(nb_8_predicted_8,nb_8_predicted_1,nb_1_predicted_1,nb_1_predicted_8)\n",
    "\n",
    "print('precision 8:', Precisionf_8, 'recall 8:', Recallf_8)\n",
    "print('precision 1:', Precisionf_1, 'recall 1:', Recallf_1)\n",
    "print('accuracy :', (Recallf_1+Recallf_8)/2)\n",
    "print('accuracy baseline:', (Recall_1+Recall_8)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoy--n6IHo3b"
   },
   "source": [
    "We improved the accuracy while reducing the embedding size from a $28\\times 28 = 784$ vector to a $4\\times 4\\times 8 = 128$ vector.\n",
    "\n",
    "We have successfully built a classifier for _eights_ and _ones_ using features extracted with a bank of pre-defined features and a set of training samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Y_mBZHv9Ho3d"
   },
   "source": [
    "## 5. Practicals: improving classification with Convolutional Neural Net\n",
    "\n",
    "You will now build a neural net that will learn the weights of the filters.\n",
    "\n",
    "The first layer of your network will be a convolutional layer with $8$ filters of size $3\\times 3$. Then you will apply a Max Pooling layer to reduce the size of the image to $4\\times 4$ as we did above. This will produce a (once flatten) a vector of size $128 = 4\\times 4\\times 8$. From this vector, you need to predict if the corresponding input is a $1$ or a $8$. So you are back to a classification problem as seen in previous lesson.\n",
    "\n",
    "You need to fill the code written below to construct your CNN. You will need to look for documentation about [torch.nn](https://pytorch.org/docs/stable/nn.html) in the Pytorch doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4ADp7YLHo3d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(classifier, self).__init__()\n",
    "        # fill the missing entries below\n",
    "        # self.lsfm   = nn.LogSoftmax(dim=1)\n",
    "        self.feature_layers = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=2)),\n",
    "            ('conv2', nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, padding=1)),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=2)),\n",
    "        ]))\n",
    "        self.fc = nn.Linear(in_features=32*7*7, out_features=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # implement your network here, use F.max_pool2d, F.log_softmax and do not forget to flatten your vector\n",
    "        # x = self.conv1(x)\n",
    "        #\n",
    "        # Your code here\n",
    "        #\n",
    "        #\n",
    "        # x = F.max_pool2d(x, kernel_size=2)  # (N, C=8, W=14, H=14)\n",
    "        # x = F.conv2d(x, input_channel=8, output_channel=32, kernel_size=3, padding=1) #(N, C=32, W=14, H=14)\n",
    "        # x = F.max_pool2d(x, kernel_size=2)  # (N, C=32, W=7, H=7)\n",
    "        # x = F.linear(x.reshape(x.shape[0], -1), input_features=32*7*7, output_features=128)\n",
    "        # x = self.fc(x) #(N, 2)\n",
    "        x = self.feature_layers(x)\n",
    "        x = self.fc(x.reshape(x.shape[0], -1))\n",
    "        x = F.log_softmax(x, dim=1) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YinNMu0Ho3g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier(\n",
      "  (feature_layers): Sequential(\n",
      "    (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "conv_class = classifier()\n",
    "print(conv_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EapjD0UQHo3k"
   },
   "source": [
    "Your code should work fine on a batch of 3 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EtN49qaUHo3p"
   },
   "outputs": [],
   "source": [
    "batch_3images = train_set.data[0:2].type(torch.FloatTensor).resize_(3, 1, 28, 28)\n",
    "#conv_class(batch_3images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 28, 28]), torch.Size([3, 8, 28, 28]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(1, 8, 3, padding=1)\n",
    "# dialation: \n",
    "# pool1 = nn.MaxPool2d(2)\n",
    "# out1 = pool1(batch_3images)\n",
    "out1 = conv1(batch_3images)\n",
    "batch_3images.shape, out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5s841xShHo3u"
   },
   "source": [
    "The following lines of code implement a data loader for the train set and the test set. No modification is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = conv_class(batch_3images)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iO_mo1CaHo3u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bs = 64\n",
    "\n",
    "l8 = np.array(0)\n",
    "eights_dataset = [[torch.from_numpy(e.astype(np.float32)).unsqueeze(0), torch.from_numpy(l8.astype(np.int64))] for e in eights]\n",
    "l1 = np.array(1)\n",
    "ones_dataset = [[torch.from_numpy(e.astype(np.float32)).unsqueeze(0), torch.from_numpy(l1.astype(np.int64))] for e in ones]\n",
    "train_dataset = eights_dataset[1000:] + ones_dataset[1000:]\n",
    "test_dataset = eights_dataset[:1000] + ones_dataset[:1000]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "    batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVzpT2LvHo3x"
   },
   "source": [
    "You need now to code the training loop. Store the loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsu7KRMIHo3y"
   },
   "outputs": [],
   "source": [
    "def train(model,data_loader,loss_fn,optimizer,n_epochs=1):\n",
    "    model.train(True)\n",
    "    loss_train = np.zeros(n_epochs)\n",
    "    acc_train = np.zeros(n_epochs)\n",
    "    for epoch_num in range(n_epochs):\n",
    "        running_corrects = 0.0\n",
    "        running_loss = 0.0\n",
    "        size = 0\n",
    "\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            bs = labels.size(0)\n",
    "            #\n",
    "            #\n",
    "            # Your code here\n",
    "            #\n",
    "            #\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss\n",
    "            preds = torch.argmax(outputs)\n",
    "            acc_es = preds == labels\n",
    "            running_corrects = torch.sum(acc_es)\n",
    "            size += bs\n",
    "        epoch_loss = running_loss.item() / size\n",
    "        epoch_acc = running_corrects.item() / size\n",
    "        loss_train[epoch_num] = epoch_loss\n",
    "        acc_train[epoch_num] = epoch_acc\n",
    "        print('Train - Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "    return loss_train, acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Es_nmD2Ho34"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'optim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_462863/4126977792.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# your SGD optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moptimizer_cl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# and train for 10 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0ml_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_cl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'optim'"
     ]
    }
   ],
   "source": [
    "conv_class = classifier()\n",
    "# choose the appropriate loss\n",
    "loss_fn = nn.NLLLoss()\n",
    "# your SGD optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer_cl = torch.optim.SGD(conv_class.parameters(), learning_rate=learning_rate)\n",
    "# and train for 10 epochs\n",
    "l_t, a_t = train(conv_class,train_loader,loss_fn,optimizer_cl,n_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPG9BbRKHo39"
   },
   "source": [
    "Let's learn for 10 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80ABV57nHo3-"
   },
   "outputs": [],
   "source": [
    "l_t1, a_t1 = train(conv_class,train_loader,loss_fn,optimizer_cl,n_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P960YqN0Ho4J"
   },
   "source": [
    "Our network seems to learn but we now need to check its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_ttkHKAHo4K"
   },
   "outputs": [],
   "source": [
    "def test(model,data_loader):\n",
    "    model.train(False)\n",
    "\n",
    "    running_corrects = 0.0\n",
    "    running_loss = 0.0\n",
    "    size = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        inputs, labels = data\n",
    "            \n",
    "        bs = labels.size(0)\n",
    "        #\n",
    "        # Your code here\n",
    "        #\n",
    "        size += bs\n",
    "\n",
    "    print('Test - Loss: {:.4f} Acc: {:.4f}'.format(running_loss / size, running_corrects.item() / size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNB9RU20Ho4M"
   },
   "outputs": [],
   "source": [
    "test(conv_class,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZUVmjBOHo4O"
   },
   "source": [
    "Change the optimizer to Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqLRDPTxHo4P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnXvEogNHo4S"
   },
   "source": [
    "How many parameters did your network learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qd5JSwZDHo4S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfxpzNo8Ho4U"
   },
   "source": [
    "You can see them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkpdG05OHo4U"
   },
   "outputs": [],
   "source": [
    "for m in conv_class.children():\n",
    "    print('weights :', m.weight.data)\n",
    "    print('bias :', m.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjXp3y5HHo4W"
   },
   "outputs": [],
   "source": [
    "for m in conv_class.children():\n",
    "    T_w = m.weight.data.numpy()\n",
    "    T_b = m.bias.data.numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-g2xTm-Ho4X"
   },
   "outputs": [],
   "source": [
    "plots([T_w[i][0] for i in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlvjNVKhHo4Z"
   },
   "outputs": [],
   "source": [
    "T_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XORiuyWzHo4a"
   },
   "source": [
    "[![Dataflowr](https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png)](https://dataflowr.github.io/website/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "06_convolution_digit_recognizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

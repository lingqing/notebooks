{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SvfKxAGCdhF"
   },
   "source": [
    "[![Dataflowr](https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png)](https://dataflowr.github.io/website/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VI1VQlwECdhG"
   },
   "source": [
    "# [Module 1](https://dataflowr.github.io/website/modules/1-intro-general-overview/): Using CNN for dogs vs cats\n",
    "\n",
    "To illustrate the Deep Learning pipeline seen in [Module 1](https://dataflowr.github.io/website/modules/1-intro-general-overview/), we are going to use a pretrained model to enter the [Dogs vs Cats](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) competition at Kaggle.\n",
    "\n",
    "[Video timestamp](https://youtu.be/ZhC-DIrCe6A?t=1175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dyhW08piCdhH"
   },
   "source": [
    "There are 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for this competition. According to the Kaggle web-site, when this competition was launched (end of 2013): *\"**State of the art**: The current literature suggests machine classifiers can score above 80% accuracy on this task\"*. So if you can beat 80%, then you will be at the cutting edge as of 2013!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4dAhx8-CdhM"
   },
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oU4Z_DPCdhM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrwJylj27Tpt"
   },
   "source": [
    "Here you see that the latest version of PyTorch is installed by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A66_r51xCdhS"
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KN3FFTFhHQyi"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuej9DPjCdhX"
   },
   "source": [
    "Check if GPU is available and if not change the [runtime](https://jovianlin.io/pytorch-with-gpu-in-google-colab/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t56d0zbFCdhY"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDLlOjT5Et4p"
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSmB5oKYCdhc"
   },
   "source": [
    "You can download the full dataset from Kaggle directly.\n",
    "\n",
    "Alternatively, Jeremy Howard (fast.ai) provides a direct link to the catvsdogs [dataset](http://files.fast.ai/data/examples/). He's separated the cats and dogs into separate folders and created a validation folder as well.\n",
    "\n",
    "For test purpose (or if you run on cpu), you should use the (small) sample directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rnn5pLK6EyJK"
   },
   "outputs": [],
   "source": [
    "%mkdir data\n",
    "# the following line should be modified if you run the notebook on your computer\n",
    "# change directory to data where you will store the dataset\n",
    "%cd /home/andy/00_workspace/dl/data\n",
    "!wget http://files.fast.ai/data/examples/dogscats.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTobJ9vTE37J"
   },
   "outputs": [],
   "source": [
    "!tar -zxvf dogscats.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsMtmnCbCdhd"
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5OW01WzCdhu"
   },
   "outputs": [],
   "source": [
    "%cd dogscats/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1vRC14z7TqI"
   },
   "source": [
    "The structure of the sub-folders inside the folder `dogscats` will be important for what follows:\n",
    "```bash\n",
    ".\n",
    "├── test1 # contains 12500 images of cats and dogs\n",
    "├── train\n",
    "|   └── cats # contains 11500 images of cats\n",
    "|   └── dogs # contains 11500 images of dogs\n",
    "├── valid\n",
    "|   └── cats # contains 1000 images of cats\n",
    "|   └── dogs # contains 1000 images of dogs\n",
    "├── sample\n",
    "|   └── train\n",
    "|       └── cats # contains 8 images of cats\n",
    "|       └── dogs # contains 8 images of dogs    \n",
    "|   └── valid \n",
    "|       └── cats # contains 4 images of cats\n",
    "|       └── dogs # contains 4 images of dogs    \n",
    "├── models # empty folder\n",
    "```\n",
    "\n",
    "You see that the 12 500 images of the test are in the `test1` sub-folder; the dataset of 25 000 labelled images has been split into a train set and a validation set.\n",
    "\n",
    "The sub-folder `sample` is here only to make sure the code is running properly on a very small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mkj3DZjpCdha"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iasXk_FKCdhy"
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6voyo_q7TqM"
   },
   "source": [
    "Below, we give the path where the data is stored. If you are running this code on your computer, you should modifiy this cell.\n",
    "\n",
    "[Video timestamp](https://youtu.be/ZhC-DIrCe6A?t=1550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJRnJgGOCdh4"
   },
   "outputs": [],
   "source": [
    "data_dir = '/spc/home/andy/00_workspace/dl/data/dogscats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3lE0cvyCdh8"
   },
   "source": [
    "`datasets` is a class of the `torchvision`| package (see [torchvision.datasets](http://pytorch.org/docs/master/torchvision/datasets.html)) and deals with data loading. It integrates a multi-threaded loader that fetches images from the disk, groups them in mini-batches and serves them continously to the GPU right after each _forward_/_backward_ pass through the network.\n",
    "\n",
    "Images needs a bit of preparation before passing them throught the network. They need to have all the same size $224\\times 224 \\times 3$ plus some extra formatting done below by the normalize transform (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8t4vokNrF19p"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "imagenet_format = transforms.Compose([\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8LMReVECdh-"
   },
   "outputs": [],
   "source": [
    "dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), imagenet_format)\n",
    "         for x in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMh7kjEBCdiC"
   },
   "outputs": [],
   "source": [
    "os.path.join(data_dir,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNyS0TyeKwdO"
   },
   "source": [
    "Interactive help on jupyter notebook thanks to `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WX5pkMX4CdiF"
   },
   "outputs": [],
   "source": [
    "?datasets.ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYmIGNEjKwdR"
   },
   "source": [
    "We see that `datasets.ImageFolder` has attributes: classes, class_to_idx, imgs.\n",
    "\n",
    "Let see what they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9ifn_R7CdiH"
   },
   "outputs": [],
   "source": [
    "dsets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vF7KOqWI7Tqf"
   },
   "source": [
    "The name of the classes are directly inferred from the structure of the folder:\n",
    "```bash\n",
    "├── train\n",
    "|   └── cats\n",
    "|   └── dogs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TB6sTwFuCdiK"
   },
   "outputs": [],
   "source": [
    "dsets['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pIheu4t7Tqk"
   },
   "source": [
    "The label 0 will correspond to cats and 1 to dogs.\n",
    "\n",
    "Below, you see that the first 5 imgs are pairs (location_of_the_image, label): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9ECrD2ACdiO"
   },
   "outputs": [],
   "source": [
    "dsets['train'].imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WefhjZb2CdiQ"
   },
   "outputs": [],
   "source": [
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'valid']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfIa-6MV7Tqp"
   },
   "source": [
    "As expected we have 23 000 images in the training set and 2 000 in the validation set.\n",
    "\n",
    "Below, we store the classes in the variable `dset_classes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCLV1YgaCdiT"
   },
   "outputs": [],
   "source": [
    "dset_classes = dsets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zy52-XhACdiX"
   },
   "source": [
    "The `torchvision` packages allows complex pre-processing/transforms of the input data (_e.g._ normalization, cropping, flipping, jittering). A sequence of transforms can be grouped in a pipeline with the help of the `torchvision.transforms.Compose` function, see [torchvision.transforms](http://pytorch.org/docs/master/torchvision/transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iphL57PhKwdh"
   },
   "source": [
    "The magic help `?` allows you to retrieve function you defined and forgot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_ARYwraCdiY"
   },
   "outputs": [],
   "source": [
    "?imagenet_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZShUSUk7Tqw"
   },
   "source": [
    "Where is this normalization coming from?\n",
    "\n",
    "As explained in the [PyTorch doc](https://pytorch.org/docs/stable/torchvision/models.html), you will use a pretrained model. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWnJQiWgGP_R"
   },
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(dsets['train'], batch_size=64, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iern_6GNCdie"
   },
   "outputs": [],
   "source": [
    "?torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wN1BKHfDCdig"
   },
   "outputs": [],
   "source": [
    "loader_valid = torch.utils.data.DataLoader(dsets['valid'], batch_size=5, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?loader_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDGO1ytx7Tq5"
   },
   "source": [
    "Try to understand what the following cell is doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4Be7lSLCdik"
   },
   "outputs": [],
   "source": [
    "count = len(loader_valid)\n",
    "inputs_try, labels_try = next(iter(loader_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BvxQqfzCdiq"
   },
   "outputs": [],
   "source": [
    "count, labels_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLNsfqc8Cdis"
   },
   "outputs": [],
   "source": [
    "inputs_try.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZK7nRSWKweH"
   },
   "source": [
    "Got it: the validation dataset contains 2 000 images, hence this is 400 batches of size 5. `labels_try` contains the labels of the first batch and `inputs_try` the images of the first batch.\n",
    "\n",
    "What is an image for your computer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrzxYMXXCdix"
   },
   "outputs": [],
   "source": [
    "inputs_try[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUjIfna97TrC"
   },
   "source": [
    "A 3-channel RGB image is of shape (3 x H x W). Note that entries can be negative because of the normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfxMdxET7TrC"
   },
   "source": [
    "A small function to display images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "346yl-gbcLLm"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "#   Imshow for Tensor.\n",
    "    # BGR to RGB\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    # denormalize\n",
    "    inp = np.clip(std * inp + mean, 0,1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJbW-QzGCdiv"
   },
   "outputs": [],
   "source": [
    "# Make a grid from batch from the validation data\n",
    "# make_grid just return one image from input mult images\n",
    "out = torchvision.utils.make_grid(inputs_try, 3)\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in labels_try])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torchvision.utils.make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HvFNWJICdiz"
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(loader_train))\n",
    "\n",
    "n_images = 8\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[0:n_images])\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOvkYiROCdi7"
   },
   "source": [
    "## Creating VGG Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BO5LAG4bCdi7"
   },
   "source": [
    "The torchvision module comes with a zoo of popular CNN architectures which are already trained on [ImageNet](http://www.image-net.org/) (1.2M training images). When called the first time, if `pretrained=True` the model is fetched over the internet and downloaded to `~/.torch/models`.\n",
    "For next calls, the model will be directly read from there.\n",
    "\n",
    "[Video timestamp](https://youtu.be/ZhC-DIrCe6A?t=2451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9PsHjXgCdi9"
   },
   "outputs": [],
   "source": [
    "# ?models.vgg16\n",
    "model_vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VBzGe2mlCdi-"
   },
   "source": [
    "We will first use VGG Model without any modification. In order to interpret the results, we need to import the 1000 ImageNet categories, available at: [https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json](https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhvs0ki_Cdi_"
   },
   "outputs": [],
   "source": [
    "%cd /home/andy/00_workspace/dl/data\n",
    "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hppljqo5CdjC"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fpath = '/home/andy/00_workspace/dl/data/imagenet_class_index.json'\n",
    "\n",
    "with open(fpath) as f:\n",
    "    class_dict = json.load(f)\n",
    "dic_imagenet = [class_dict[str(i)][1] for i in range(len(class_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QATvAFILCdjF"
   },
   "outputs": [],
   "source": [
    "dic_imagenet[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6QQhwruCdjI"
   },
   "outputs": [],
   "source": [
    "inputs_try , labels_try = inputs_try.to(device), labels_try.to(device)\n",
    "\n",
    "model_vgg = model_vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epMB0UF9CdjM"
   },
   "outputs": [],
   "source": [
    "outputs_try = model_vgg(inputs_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOlx7YcPCdjO"
   },
   "outputs": [],
   "source": [
    "outputs_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrweFfK9k0TW"
   },
   "outputs": [],
   "source": [
    "outputs_try.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nk538XVpHc0y"
   },
   "source": [
    "To translate the outputs of the network into 'probabilities', we pass it through a [Softmax function](https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCIxHN2QCdjT"
   },
   "outputs": [],
   "source": [
    "m_softm = nn.Softmax(dim=1)\n",
    "probs = m_softm(outputs_try)\n",
    "vals_try,preds_try = torch.max(probs,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xB5U4_mA7Trf"
   },
   "source": [
    "Let check, that we obtain a probability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGTVkH2pk0Tb"
   },
   "outputs": [],
   "source": [
    "torch.sum(probs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvGbb2bQCdjZ"
   },
   "outputs": [],
   "source": [
    "vals_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TUdNKCJCdjc"
   },
   "outputs": [],
   "source": [
    "print([dic_imagenet[i] for i in preds_try.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtzuHob9Cdjf"
   },
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(inputs_try.data.cpu())\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in labels_try.data.cpu()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3Bktt5PCdjh"
   },
   "source": [
    "### Modifying the last layer and setting the gradient false to all layers\n",
    "\n",
    "[Video timestamp](https://youtu.be/ZhC-DIrCe6A?t=2755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L8kr3-tjCdji"
   },
   "outputs": [],
   "source": [
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXoMW73MCdjl"
   },
   "source": [
    "We'll learn about what these different blocks do later in the course. For now, it's enough to know that:\n",
    "\n",
    "- Convolution layers are for finding small to medium size patterns in images -- analyzing the images locally\n",
    "- Dense (fully connected) layers are for combining patterns across an image -- analyzing the images globally\n",
    "- Pooling layers downsample -- in order to reduce image size and to improve invariance of learned features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hA2f5FRuCdjm"
   },
   "source": [
    "![vgg16](https://dataflowr.github.io/notebooks/Module1/img/vgg16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SK6lfAzfCdjn"
   },
   "source": [
    "In this practical example, our goal is to use the already trained model and just change the number of output classes. To this end we replace the last `nn.Linear` layer trained for 1000 classes to ones with 2 classes. In order to freeze the weights of the other layers during training, we set the field `required_grad=False`. In this manner no gradient will be computed for them during backprop and hence no update in the weights. Only the weights for the 2 class layer will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQwRKKC-Cdjo"
   },
   "outputs": [],
   "source": [
    "for param in model_vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "model_vgg.classifier._modules['6'] = nn.Linear(4096, 2)\n",
    "model_vgg.classifier._modules['7'] = torch.nn.LogSoftmax(dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAr5tj2NKwfF"
   },
   "source": [
    "PyTorch documentation for [LogSoftmax](https://pytorch.org/docs/stable/nn.html#logsoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJ3OenJpCdjp"
   },
   "outputs": [],
   "source": [
    "print(model_vgg.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfX1VctZ7Trx"
   },
   "source": [
    "We load the model on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ITZFX2MCdju"
   },
   "outputs": [],
   "source": [
    "model_vgg = model_vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fU9vWWT2CdkN"
   },
   "source": [
    "## Training the fully connected module\n",
    "\n",
    "[Video timestamp](https://youtu.be/ZhC-DIrCe6A?t=2990)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqp2u3IXCdkO"
   },
   "source": [
    "### Creating loss function and optimizer\n",
    "\n",
    "PyTorch documentation for [NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss) and the [torch.optim module](https://pytorch.org/docs/stable/optim.html#module-torch.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oP1F4yb8CdkO"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer_vgg = torch.optim.SGD(model_vgg.classifier[6].parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tenuLj67CdkS"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nNAUibjCdkS"
   },
   "outputs": [],
   "source": [
    "def train_model(model,dataloader,size,epochs=1,optimizer=None):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs,classes in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            classes = classes.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,classes)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _,preds = torch.max(outputs.data,1)\n",
    "            # statistics\n",
    "            running_loss += loss.data.item()\n",
    "            running_corrects += torch.sum(preds == classes.data)\n",
    "        epoch_loss = running_loss / size\n",
    "        epoch_acc = running_corrects.data.item() / size\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                     epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jts2jK1CdkV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model_vgg,loader_train,size=dset_sizes['train'],epochs=2,optimizer=optimizer_vgg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **note**  \n",
    "    * softmax + log = logsoftmax\n",
    "    * logsoftmax + NLLLoss = crossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkMYIX21Hc1k"
   },
   "outputs": [],
   "source": [
    "def test_model(model,dataloader,size):\n",
    "    model.eval()\n",
    "    predictions = np.zeros(size)\n",
    "    all_classes = np.zeros(size)\n",
    "    all_proba = np.zeros((size,2))\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for inputs,classes in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,classes)           \n",
    "        _,preds = torch.max(outputs.data,1)\n",
    "            # statistics\n",
    "        running_loss += loss.data.item()\n",
    "        running_corrects += torch.sum(preds == classes.data)\n",
    "        predictions[i:i+len(classes)] = preds.to('cpu').numpy()\n",
    "        all_classes[i:i+len(classes)] = classes.to('cpu').numpy()\n",
    "        all_proba[i:i+len(classes),:] = outputs.data.to('cpu').numpy()\n",
    "        i += len(classes)\n",
    "    epoch_loss = running_loss / size\n",
    "    epoch_acc = running_corrects.data.item() / size\n",
    "    print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                     epoch_loss, epoch_acc))\n",
    "    return predictions, all_proba, all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3JjYayKCdkW"
   },
   "outputs": [],
   "source": [
    "predictions, all_proba, all_classes = test_model(model_vgg,loader_valid,size=dset_sizes['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cn6cIwZNCdkY"
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(loader_valid))\n",
    "\n",
    "out = torchvision.utils.make_grid(inputs[0:n_images])\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ghkbm7ByCdke"
   },
   "outputs": [],
   "source": [
    "outputs = model_vgg(inputs[:n_images].to(device))\n",
    "print(torch.exp(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1qdUtvjCdkg"
   },
   "outputs": [],
   "source": [
    "classes[:n_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0n0hLxTHc1w"
   },
   "source": [
    "## Speeding up the learning by precomputing features\n",
    "\n",
    "[Video timestamp](https://youtu.be/ZhC-DIrCe6A?t=3460)\n",
    "\n",
    "Here you are wasting a lot of time computing over and over the same quantities. Indeed, the first part of the VGG model (called `features` and made of convolutional layers) is frozen and never updated. Hence, we can precompute for each image in the dataset, the output of these convolutional layers as these outputs will always be the same during your training process.\n",
    "\n",
    "This is what is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39AALjfgHc1y"
   },
   "outputs": [],
   "source": [
    "x_try = model_vgg.features(inputs_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwjaBq8lHc11"
   },
   "outputs": [],
   "source": [
    "x_try.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfCTEBTm7TsF"
   },
   "source": [
    "You see that the features computed for an image is of shape 512x7x7 (above we have a batch corresponding to 5 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZegyLN3Hc13"
   },
   "outputs": [],
   "source": [
    "def preconvfeat(dataloader):\n",
    "    conv_features = []\n",
    "    labels_list = []\n",
    "    for data in dataloader:\n",
    "        inputs,labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        x = model_vgg.features(inputs)\n",
    "        conv_features.extend(x.data.cpu().numpy())\n",
    "        labels_list.extend(labels.data.cpu().numpy())\n",
    "    conv_features = np.concatenate([[feat] for feat in conv_features])\n",
    "    return (conv_features,labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQiycgsqHc1_"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "conv_feat_train,labels_train = preconvfeat(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imSeW6-7Hc2D"
   },
   "outputs": [],
   "source": [
    "conv_feat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3e5bjsuHc2F"
   },
   "outputs": [],
   "source": [
    "# %time 的计算结果包括：CPU time（CPU运行程序的时间）， Wall time（Wall Clock Time，墙上挂钟的时间，也就是我们感受到的运行时间）。\n",
    "# '%timeit 计时更为精确，这一命令会运行代码 r 次，每次 n 遍，再对 n*r 遍的结果取平均后，得到运行一遍代码的时间。\n",
    "# '%%time 或 %%timeit：计算当前单元(cell)的代码运行时间\n",
    "%%time\n",
    "conv_feat_valid,labels_valid = preconvfeat(loader_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0fpZByOHc2H"
   },
   "source": [
    "### Creating a new data generator\n",
    "\n",
    "We will not load images anymore, so we need to build our own data loader. If you do not understand the cell below, it is OK! We will come back to it in Lesson 5..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpx-uU90Hc2H"
   },
   "outputs": [],
   "source": [
    "dtype=torch.float\n",
    "## Why?, generate dataloader, which contains inputs [here are feature] and labels\n",
    "datasetfeat_train = [[torch.from_numpy(f).type(dtype),torch.tensor(l).type(torch.long)] for (f,l) in zip(conv_feat_train,labels_train)]\n",
    "datasetfeat_train = [(inputs.reshape(-1), classes) for [inputs,classes] in datasetfeat_train]\n",
    "loaderfeat_train = torch.utils.data.DataLoader(datasetfeat_train, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFNC9PyGHc2P"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model_vgg.classifier,dataloader=loaderfeat_train,size=dset_sizes['train'],epochs=50,optimizer=optimizer_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykG_Rld97TsO"
   },
   "outputs": [],
   "source": [
    "datasetfeat_valid = [[torch.from_numpy(f).type(dtype),torch.tensor(l).type(torch.long)] for (f,l) in zip(conv_feat_valid,labels_valid)]\n",
    "datasetfeat_valid = [(inputs.reshape(-1), classes) for [inputs,classes] in datasetfeat_valid]\n",
    "loaderfeat_valid = torch.utils.data.DataLoader(datasetfeat_valid, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yg770yZHc2W"
   },
   "outputs": [],
   "source": [
    "predictions, all_proba, all_classes = test_model(model_vgg.classifier,dataloader=loaderfeat_valid,size=dset_sizes['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlBOnslWHc2Y"
   },
   "source": [
    "## 4. Viewing model prediction (qualitative analysis)\n",
    "\n",
    "[Video timestamp](https://youtu.be/ZhC-DIrCe6A?t=3819)\n",
    "\n",
    "The most important metrics for us to look at are for the validation set, since we want to check for over-fitting.\n",
    "\n",
    "With our first model we should try to overfit before we start worrying about how to handle that - there's no point even thinking about regularization, data augmentation, etc if you're still under-fitting! (We'll be looking at these techniques after the 2 weeks break...)\n",
    "\n",
    "\n",
    "As well as looking at the overall metrics, it's also a good idea to look at examples of each of:\n",
    "\n",
    "   1. A few correct labels at random\n",
    "   2. A few incorrect labels at random\n",
    "   3. The most correct labels of each class (ie those with highest probability that are correct)\n",
    "   4. The most incorrect labels of each class (ie those with highest probability that are incorrect)\n",
    "   5. The most uncertain labels (ie those with probability closest to 0.5).\n",
    "\n",
    "In general, these are particularly useful for debugging problems in the model. Since our model is very simple, there may not be too much to learn at this stage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJr5EysXHc2a"
   },
   "outputs": [],
   "source": [
    "# Number of images to view for each visualization task\n",
    "n_view = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_hokobPHc2i"
   },
   "outputs": [],
   "source": [
    "correct = np.where(predictions==all_classes)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4Y8kRv-Hc2o"
   },
   "outputs": [],
   "source": [
    "len(correct)/dset_sizes['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72LZk1rXHc2v"
   },
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "idx = permutation(correct)[:n_view]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wrel6SmsHc2z"
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQ2pQ7p_Hc22"
   },
   "outputs": [],
   "source": [
    "loader_correct = torch.utils.data.DataLoader([dsets['valid'][x] for x in idx],batch_size = n_view,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-Phdh-tHc25"
   },
   "outputs": [],
   "source": [
    "for data in loader_correct:\n",
    "    inputs_cor,labels_cor = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PIV_a2XHc27"
   },
   "outputs": [],
   "source": [
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs_cor)\n",
    "\n",
    "imshow(out, title=[l.item() for l in labels_cor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8hWSmjJHc2-"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "for x in idx:\n",
    "    display(Image(filename=dsets['valid'].imgs[x][0], retina=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXx3REbpHc3B"
   },
   "outputs": [],
   "source": [
    "incorrect = np.where(predictions!=all_classes)[0]\n",
    "for x in permutation(incorrect)[:n_view]:\n",
    "    #print(dsets['valid'].imgs[x][1])\n",
    "    display(Image(filename=dsets['valid'].imgs[x][0], retina=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh7QdEVZHc3E"
   },
   "outputs": [],
   "source": [
    "#3. The images we most confident were cats, and are actually cats\n",
    "correct_cats = np.where((predictions==0) & (predictions==all_classes))[0]\n",
    "# argsort:\n",
    "most_correct_cats = np.argsort(all_proba[correct_cats,1])[:n_view]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocYE1fvHHc3I"
   },
   "outputs": [],
   "source": [
    "for x in most_correct_cats:\n",
    "    display(Image(filename=dsets['valid'].imgs[correct_cats[x]][0], retina=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmZkwPEsHc3O"
   },
   "outputs": [],
   "source": [
    "#3. The images we most confident were dogs, and are actually dogs\n",
    "correct_dogs = np.where((predictions==1) & (predictions==all_classes))[0]\n",
    "most_correct_dogs = np.argsort(all_proba[correct_dogs,0])[:n_view]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8jUtIj4Hc3R"
   },
   "outputs": [],
   "source": [
    "for x in most_correct_dogs:\n",
    "    display(Image(filename=dsets['valid'].imgs[correct_dogs[x]][0], retina=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjwIo3gbsGMx"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "What did we do in the end? A simple logistic regression! If the connection is unclear, we'll explain it on a much simpler example in the next course. \n",
    "\n",
    "We probably killed a fly with a sledge hammer!\n",
    "\n",
    "In our case, the sledge hammer is VGG pretrained on Imagenet, a dataset containing a lot of pictures of cats and dogs. Indeed, we saw that without modification the network was able to predict dog and cat breeds. Hence it is not very surprising that the features computed by VGG are very accurate for our classification task. In the end, we need to learn only the parameters of the last linear layer, i.e. 8194 parameters (do not forget the bias $2\\times 4096+2$). Indeed, this can be done on CPU without any problem.\n",
    "\n",
    "Nevertheless, this example is still instructive as it shows all the necessary steps in a deep learning project. Here we did not struggle with the learning process of a deep network, but we did all the preliminary engineering tasks: dowloading a dataset, setting up the environment to use a GPU, preparing the data, computing the features with a pretrained VGG, saving them on your drive so that you can use them for a later experiment... These steps are essential in any deep learning project and a necessary requirement before having fun playing with network architectures and understanding the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SvfKxAGCdhF"
   },
   "source": [
    "[![Dataflowr](https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png)](https://dataflowr.github.io/website/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "01_intro.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f014e20b5d4b009dfa10215228b9ab915030234f416ea935b9092a5da498cfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
